{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f52ab5-ad75-4435-bb49-c87a3781b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114775b4891e4a1482b136aa782d3181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='5667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/5667 12:00:09 < 55:59:32, 0.02 it/s, Epoch 0.53/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.512200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Загрузка модели в 4-битном формате\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,         # Использование вложенной квантизации для дополнительной экономии памяти\n",
    "    bnb_4bit_quant_type=\"nf4\",              # Метод квантизации: NormalFloat4\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.to('cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Загрузка датасета\n",
    "ds = load_dataset(\"ScalableMath/Lean-STaR-plus\")\n",
    "ds = ds['train'].train_test_split(test_size=0.01, seed=42)\n",
    "ds_train = ds['train']\n",
    "ds_test = ds['test']\n",
    "\n",
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"text\": f\"{example['input']}\\n{example['output']}\"\n",
    "    }\n",
    "\n",
    "formatted_ds = ds_train.map(format_instruction)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_ds = formatted_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Определение конфигурации LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Настройка параметров обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-math-qlora\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=20,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Инициализация Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение адаптера и токенизатора\n",
    "model.save_pretrained(\"./qwen-math-qlora\")\n",
    "tokenizer.save_pretrained(\"./qwen-math-qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8eff96-18a2-4867-b78f-6066aeb8708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed53edc8b124e4ab3a6f1c38bf6be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Загрузка конфигурации адаптера\n",
    "config = PeftConfig.from_pretrained(\"./qwen-math-qlora/checkpoint-1000\")\n",
    "\n",
    "# Загрузка базовой модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Применение QLoRA адаптера\n",
    "model = PeftModel.from_pretrained(model, \"./qwen-math-qlora/checkpoint-1000\")\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./qwen-math-qlora/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99dbfb40-4ce5-42eb-8d9f-a259591b4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "My LEAN 4 state is:\n",
      "import Mathlib.Data.Matrix.Basic\n",
      "import Aesop\n",
      "import Mathlib.Tactic.NormNum\n",
      "import Mathlib.Tactic.RewriteSearch\n",
      "import AutoSolver\n",
      "\n",
      "\n",
      "@[simp] def x: List ℚ :=  [6, 2, 9]\n",
      "@[simp] def e_1: List ℚ :=  [5, 0, 4]\n",
      "@[simp] def e_2: List ℚ :=  [5, -1, 0]\n",
      "@[simp] def e_3: List ℚ :=  [-1, 0, 4]\n",
      "\n",
      "@[simp] def x1: ℚ := 73/24\n",
      "@[simp] def x2: ℚ := -2\n",
      "@[simp] def x3: ℚ := -19/24\n",
      "Please write down the reasoning that leads to the possible next tactic and then predict the tactic to help me prove the corectness of the system. assistant\n",
      "\n",
      "Here is the reasoning:\n",
      "To prove the goal, we need to show that the given list of rational numbers is a basis for the vector space. This can be achieved by using the fact that the list of rational numbers is linearly independent and spans the vector space.\n",
      "Here is the predicted next tactic:\n",
      "```lean\n",
      "exact Aesop.basis (Aesop.quotient 3) (Aesop.quotient 3) x e_1 e_2 e_3\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Пример генерации\n",
    "input_text = \"\"\"<|im_start|>user\n",
    "My LEAN 4 state is:\n",
    "import Mathlib.Data.Matrix.Basic\n",
    "import Aesop\n",
    "import Mathlib.Tactic.NormNum\n",
    "import Mathlib.Tactic.RewriteSearch\n",
    "import AutoSolver\n",
    "\n",
    "\n",
    "@[simp] def x: List ℚ :=  [6, 2, 9]\n",
    "@[simp] def e_1: List ℚ :=  [5, 0, 4]\n",
    "@[simp] def e_2: List ℚ :=  [5, -1, 0]\n",
    "@[simp] def e_3: List ℚ :=  [-1, 0, 4]\n",
    "\n",
    "@[simp] def x1: ℚ := 73/24\n",
    "@[simp] def x2: ℚ := -2\n",
    "@[simp] def x3: ℚ := -19/24\n",
    "Please write down the reasoning that leads to the possible next tactic and then predict the tactic to help me prove the corectness of the system.<|im_end|> \\\n",
    "<|im_start|>assistant\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**input_ids, max_length=512, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096bcf6-11ea-4284-828b-da7a1541c7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aogavrilov-qwen-enc)",
   "language": "python",
   "name": "venv_qwen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
